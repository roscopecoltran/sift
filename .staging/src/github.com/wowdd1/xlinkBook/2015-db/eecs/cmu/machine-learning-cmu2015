MLG10500 | Senior Research Project | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10500&SEMESTER=M15 | description:Register for this course if you are minoring in Machine Learning. This course is intended for research with a faculty member that would count towards the minor.
MLG10601 | Introduction to Machine Learning | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10601&SEMESTER=S15 | instructors:Rosenfeld, Ronald prereq:(15122) and (21127 or 15151) description:Machine Learning (ML) develops computer programs that automatically improve their performance through experience.   This includes learning many types of tasks based on many types of experience, e.g. spotting high-risk medical patients, recognizing speech, classifying text documents, detecting credit card fraud, or driving autonomous vehicles.  10601 covers all or most of: concept learning, decision trees, neural networks, linear learning, active learning, estimation & the bias-variance tradeoff, hypothesis testing, Bayesian learning, the MDL principle, the Gibbs classifier, Naive Bayes, Bayes Nets & Graphical Models, the EM algorithm, Hidden Markov Models, K-Nearest-Neighbors and nonparametric learning, reinforcement learning, bagging, boosting and discriminative training.  Grading will be based on weekly or biweekly assignments (written and/or programming), a midterm, a final exam, and possibly a project (details may vary depending on the section).  10601 is recommended for CS Seniors & Juniors, quantitative Masters students, & non-MLD PhD students.    Prerequisites (strictly enforced): strong quantitative aptitude, college prob&stats course, and programming proficiency.  For learning to apply ML practically & effectively, without the above prerequisites, consider 11344/05834 instead.  You can evaluate your ability to take the course via a self-assessment exam at: http://www.cs.cmu.edu/~aarti/Class/10701_Spring14/Intro_ML_Self_Evaluation.pdf For section-specific information, see:Section 10601A: https://sites.google.com/site/10601a14spring/about-the-classSection 10601B:
MLG10605 | Machine Learning with Large Datasets | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10605&SEMESTER=S15 | instructors:Cohen, William prereq:15210 or 15214 description:Large datasets are difficult to work with for several reasons. They are difficult to visualize, and it is difficult to understand what sort of errors and biases are present in them. They are computationally expensive to process, and often the cost of learning is hard to predict - for instance, and algorithm that runs quickly in a dataset that fits in memory may be exorbitantly expensive when the dataset is too large for memory. Large datasets may also display qualitatively different behavior in terms of which learning methods produce the most accurate predictions.  This course is intended to provide a student practical knowledge of, and experience with, the issues involving large datasets. Among the issues considered are: scalable learning techniques, such as streaming machine learning techniques; parallel infrastructures such as map-reduce; practical techniques for reducing the memory requirements for learning methods, such as feature hashing and Bloom filters; and techniques for analysis of programs in terms of memory, disk usage, and (for parallel methods) communication complexity.  The class will include programming assignments, and a one-month short project chosen by the student. The project will be designed to compare the scalability of variant learning algorithms on datasets.  An introductory course in machine learning, like 10-601 or 10-701, is a prerequisite or a co-requisite. If you plan to take this course and 10-601 concurrently please tell the instructor.  The course will include several substantial programming assignments, so an additional prerequisite is 15-211, or 15-214, or comparable familiarity with Java and good programming skills.  Undergraduates need permission of the instructor to enroll.
MLG10611 | MS Data Analysis Project | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10611&SEMESTER=M15 | description:This course is for ML Masters students to work on research with their advisor for the Data Analysis Project.
MLG10620 | Independent Study: Research | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10620&SEMESTER=S15 | description:None
MLG10697 | Reading and Research | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10697&SEMESTER=S15 | description:Course for MS students to work with their advisor on research
MLG10701 | Introduction to Machine Learning | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10701&SEMESTER=S15 | instructors:Smola, Alexander description:Machine learning studies the question "How can we build computer programs that automatically improve their performance through experience?"   This includes learning to perform many types of tasks based on many types of experience.  For example, it includes robots learning to better navigate based on experience gained by roaming their environments, medical decision aids that learn to predict which therapies work best for which diseases based on data mining of historical health records, and speech recognition systems that lean to better understand your speech based on experience listening to you.  This course is designed to give PhD students a thorough grounding in the methods, mathematics and algorithms needed to do research and applications in machine learning. Students entering the class with a pre-existing working knowledge of probability, statistics and algorithms will be at an advantage, but the class has been designed so that anyone with a strong numerate background can catch up and fully participate. You can evaluate your ability to take the course via a self-assessment exam that will be made available to you  after you register.  If you are interested in this topic, but are not a PhD student, or are a PhD student not specializing in machine learning, you might consider the master's level course on Machine Learning, 10-601."  This class may be appropriate for MS and undergrad students who are interested in the theory and algorithms behind ML.  You can evaluate your ability to take the course via a self-assessment exam at: http://www.cs.cmu.edu/~aarti/Class/10701_Spring14/Intro_ML_Self_Evaluation.pdf
MLG10702 | Statistical Machine Learning | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10702&SEMESTER=S15 | instructors:Wasserman, Larry Tibshirani, Ryan prereq:(10705 or 36705) and (10701 or 10715) description:Statistical Machine Learning is a second graduate level course in advanced machine learning, assuming that students have taken Machine Learning (10-701) or Advanced Machine Learning (10-715), and Intermediate Statistics (36-705). The term ?statistical? in the title reflects the emphasis on statistical theory and methodology.This course is mostly focused on methodology and theoretical foundations. It treats both the ?art? of designing good learning algorithms and the ?science? of analyzing an algorithm?s statistical properties and performance guarantees. Theorems are presented together with practical aspects of methodology and intuition to help students develop tools for selecting appropriate methods and approaches to problems in their own research.  Though computation is certainly a critical component of what makes a method successful, it will not receive the same central focus as methodology and theory. We will cover topics in statistical theory that are important for researchers in machine learning, including consistency, minimax estimation, and concentration of measure. We will also cover statistical topics that may not be covered in as much depth in other machine learning courses, such as nonparametric density estimation, nonparametric regression, and Bayesian estimation.
MLG10704 | Information Processing and Learning | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10704&SEMESTER=S15 | instructors:Singh, Aarti Krishnamurthy, Akshay description:What's the connection between how many bits we can send over a channel and how accurately we can classify documents or fit a curve to data? Is there any link between decision trees, prefix codes and wavelet transforms? What about max-entropy and maximum likelihood, or universal coding and online learning? This inter-disciplinary course will explore these and other questions that link the fields of information theory, signal processing, and machine learning, all of which aim to understand the information contained in data. The goal is to highlight the common concepts and establish concrete links between these fields that enable efficient information processing and learning.Unlike the last offering of the course (url below) which covered basics of information theory in detail, this time we will do a short but introductory review of basic information theory, including entropy and fundamental limits of data compression, data processing and Fano's inequalities, channel capacity, and rate-distortion theory. Then we will dig into the connections to learning including: estimation of information theoretic quantities (such entropy, mutual information, and divergence) and their applications in learning, information theoretic lower bounds for machine learning problems, duality of max entropy and maximum likelihood, connections between clustering and rate-distortion theory, universal coding and online learning, active learning and feedback channels, and more.We expect that the revamped course will cater to both students that have taken a basic information theory course and those that have not. Pre-requisites: Fundamentals of Probability, Statistics, Linear Algebra and Real analysis.
MLG10705 | Intermediate Statistics | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10705&SEMESTER=F14 | instructors:Wasserman, Larry description:This course covers the fundamentals of theoretical statistics. Topics include: probability inequalities, point and interval estimation, minimax theory, hypothesis testing, data reduction, convergence concepts, Bayesian inference, nonparametric statistics, bootstrap resampling, VC dimension, prediction and model selection.
MLG10708 | Probabilistic Graphical Models | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10708&SEMESTER=S15 | instructors:Xing, Poe prereq:10701 or 15781 description:Many of the problems in artificial intelligence, statistics, computer systems, computer vision, natural language processing, and computational biology, among many other fields, can be viewed as the search for a coherent global conclusion from local information. The probabilistic graphical models framework provides an unified view for this wide range of problems, enabling efficient inference, decision-making and learning in problems with a very large number of attributes and huge datasets. This graduate-level course will provide you with a strong foundation for both applying graphical models to complex problems and for addressing core research topics in graphical models.  The class will cover three aspects: The core representation, including Bayesian and Markov networks, and dynamic Bayesian networks; probabilistic inference algorithms, both exact and approximate; and, learning methods for both the parameters and the structure of graphical models. Students entering the class should have a pre-existing working knowledge of probability, statistics, and algorithms, though the class has been designed to allow students with a strong numerate background to catch up and fully participate.  It is expected that after taking this class, the students should have obtain sufficient working knowledge of multi-variate probabilistic modeling and inference for practical applications, should be able to formulate and solve a wide range of problems in their own domain using GM, and can advance into more specialized technical literature by themselves.  Students are required to have successfully completed 10701/15781, or an equivalent class.
MLG10715 | Advanced Introduction to Machine Learning | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10715&SEMESTER=F14 | instructors:Poczos, Barnabas Xing, Poe description:The rapid improvement of sensory techniques and processor speed, and the availability of inexpensive massive digital storage, have led to a growing demand for systems that can automatically comprehend and mine massive and complex data from diverse sources.  Machine Learning is becoming the primary mechanism by which information is extracted from Big Data, and a primary pillar that Artificial Intelligence is built upon.  This course is designed for Ph.D. students whose primary field of study is machine learning, or who intend to make machine learning methodological research a main focus of their thesis.  It will give students a thorough grounding in the algorithms, mathematics, theories, and insights needed to do in-depth research and applications in machine learning. The topics of this course will in part parallel those covered in the general graduate machine learning course (10-701), but with a greater emphasis on depth in theory and algorithms.   The course will also include additional advanced topics such as RKHS and representer theory, Bayesian nonparametrics, additional material on graphical models, manifolds and spectral graph theory, reinforcement learning and online learning, etc.  Students entering the class are expected to have a pre-existing strong working knowledge of algorithms, linear algebra, probability, and statistics.  If you are interested in this topic, but do not have the required background or are not planning to work on a PhD thesis with machine learning as the main focus, you might consider the general graduate Machine Learning course (10-701) or the Masters-level Machine Learning course (10-601).
MLG10725 | Convex Optimization | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10725&SEMESTER=S15 | instructors:Tibshirani, Ryan description:Nearly every problem in computer science and statistics can be formulated as the optimization of some function, possibly under some set of constraints.  This universal reduction may seem to suggest that such optimization tasks are intractable. Fortunately, many real world problems have special structure, such as convexity, smoothness, separability, etc., which allow us to formulate optimization problems that can often be solved efficiently. This course is designed to give a graduate-level student a thorough grounding in the formulation of optimization problems that exploit such structure, and in efficient solution methods for these problems. The main focus is on the formulation and solution of convex optimization problems. These general concepts will also be illustrated through applications in machine learning and statistics.  Students entering the class should have a pre-existing working knowledge of algorithms, though the class has been designed to allow students with a strong numerate background to catch up and fully participate. Though not required, having taken 10-701 or an equivalent machine learning or statistics class is strongly encouraged, since we will use applications in machine learning and statistics to demonstrate the concepts we cover in class.  Students will work on an extensive optimization-based project throughout the semester; those wanting to take the class without the project can register under the 9 unit option.
MLG10805 | Machine Learning with Large Datasets | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10805&SEMESTER=S15 | instructors:Cohen, William description:Large datasets are difficult to work with for several reasons. They are difficult to visualize, and it is difficult to understand what sort of errors and biases are present in them. They are computationally expensive to process, and often the cost of learning is hard to predict - for instance, and algorithm that runs quickly in a dataset that fits in memory may be exorbitantly expensive when the dataset is too large for memory. Large datasets may also display qualitatively different behavior in terms of which learning methods produce the most accurate predictions.  This course is intended to provide a student practical knowledge of, and experience with, the issues involving large datasets. Among the issues considered are: scalable learning techniques, such as streaming machine learning techniques; parallel infrastructures such as map-reduce; practical techniques for reducing the memory requirements for learning methods, such as feature hashing and Bloom filters; and techniques for analysis of programs in terms of memory, disk usage, and (for parallel methods) communication complexity.  An introductory course in machine learning, like 10-601 or 10-701, is a prerequisite or a co-requisite.     The class will include programming assignments, presentation of relevant research papers to the class, and a research project chosen by the student, to be presented to the class, and written up in a conference-paper format.  10-805 will share lectures with 10-605, but 10-805 students need to make class presentations and complete a research project, and will do fewer programming assignments, so 10-805 students are expected to be capable of surveying recent literature and conducting research. Four lecture sessions for 10-605 will also be reserved for 10-805 students' presentations.  If there is sufficient interest we will introduce a mechanism for 10-605 students to collaborate of 10-805 students on projects.
MLG10910 | Directed Research | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10910&SEMESTER=M15 | description:Data Analysis Project research done with your advisor.
MLG10915 | MLD Journal Club | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10915&SEMESTER=S15 | instructors:Maxion, Roy description:This course provides a forum for students in Machine Learning to practice public speaking and technical reading skills. In addition, it will provide a venue for satisfying the MLD speaking requirement as well as the oral part of the Data Analysis Project. All requirements talks will be open to the public and advertised on the relevant seminar lists. During a typical meeting, one of the students will present a 20-30 minute talk and answer questions; then, the remaining students and the instructors will give advice for improving the talk and responding better to questions.  Presentations can be based on a research paper or papers from the literature, or on the student's own work, or a combination of the two. Students will present at least once or twice before attempting to pass the speaking requirement or DAP oral requirement.  The course will include brief workshops embedded throughout the semester to cover such things as: effective structure of presentations, how to give a short talk (think NIPS spotlights), "elevator" talks, structure of a research paper, conference presentations, proposal writing (think thesis and beyond), slide crafting, posters, critical evaluation, and public communications for research.  The talk schedule for the semester will be largely formed during the first course meeting, so students who cannot attend the first meeting are strongly encouraged to contact the instructors ahead of time. All students wishing to satisfy the speaking or DAP oral requirement in a given semester must register for this course. In addition, this course is required for all first-year MLD students.
MLG10920 | Graduate Reading and Research | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10920&SEMESTER=M15 | description:This course is for graduate students to work on research with their advisor before they propose their thesis topic.
MLG10930 | Dissertation Research | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10930&SEMESTER=M15 | description:This course is for graduate students to work on their dissertation research after they have proposed their thesis topic.
MLG10935 | Practicum | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10935&SEMESTER=M15 | description:None
MLG10940 | Independent Study | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10940&SEMESTER=S15 | description:Independent Study to be used to work on research with faculty.
