mit-media-fluid-interfaces-1 | Augmented Airbrush | http://fluid.media.mit.edu/projects/digital-airbrush | description:Roy Shilkrot, Amit Zoran, Pattie Maes and Joseph A. Paradiso We present an augmented handheld airbrush that allows unskilled painters to experience the art of spray painting. Inspired by similar smart tools for fabrication, our handheld device uses 6DOF tracking, mechanical augmentation of the airbrush trigger, and a specialized algorithm to let the painter apply color only where indicated by a reference image. It acts both as a physical spraying device and as an intelligent digital guiding tool that provides manual and computerized control. Using an inverse rendering approach allows for a new augmented painting experience with unique results. We present our novel hardware design, control software, and a discussion of the implications of human-computer collaborative painting. view site
mit-media-fluid-interfaces-2 | CarDio | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-fluid-interfaces-2 CarDio | description:Special Interest group(s):  Advancing Wellbeing Chang Long Zhu Jin, Judith Amores Fernandez, Xavier Benavides Palos, Roger Boldu Busquets and Pattie Maes We are designing interfaces to enhance driver self-awareness through subliminal visual feedback and shape-changing materials. Advancements in sensing technologies make it possible to measure physiological data in the car environment, opening up the possibility of harnessing such data for just-in-time feedback to drivers.
mit-media-fluid-interfaces-3 | Enlight | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-fluid-interfaces-3 Enlight | description:Tal Achituv, Natan Linder, Rony Kubat, Pattie Maes and Yihui Saw, In physics education, virtual simulations have given us the ability to show and explain phenomena that are otherwise invisible to the naked eye. However, experiments with analog devices still play an important role. They allow us to verify theories and discover ideas through experiments that are not constrained by software. What if we could combine the best of both worlds? We achieve that by building our applications on a projected augmented reality system. By projecting onto physical objects, we can paint the phenomena that are invisible. With our system, we have built "physical playgrounds"—simulations that are projected onto the physical world and that respond to detected objects in the space. Thus, we can draw virtual field lines on real magnets, track and provide history on the location of a pendulum, or even build circuits with both physical and virtual components.
mit-media-fluid-interfaces-4 | EyeRing: A Compact, Intelligent Vision System on a Ring | http://fluid.media.mit.edu/projects/eyering | description:Roy Shilkrot and Suranga Nanayakkara EyeRing is a wearable, intuitive interface that allows a person to point at an object to see or hear more information about it. We came up with the idea of a micro-camera worn as a ring on the index finger with a button on the side, which can be pushed with the thumb to take a picture or a video that is then sent wirelessly to a mobile phone to be analyzed. The user tells the system what information they are interested in and receives the answer in either auditory or visual form. The device also provides some simple haptic feedback. This finger-worn configuration of sensors and actuators opens up a myriad of possible applications for the visually impaired as well as for sighted people. view site
mit-media-fluid-interfaces-5 | FingerReader | http://fluid.media.mit.edu/projects/fingerreader | description:Roy Shilkrot, Jochen Huber, Pattie Maes and Suranga Nanayakkara FingerReader is a finger-worn device that helps the visually impaired to effectively and efficiently read paper-printed text. It works in a local-sequential manner for scanning text that enables reading of single lines or blocks of text, or skimming the text for important sections while providing auditory and haptic feedback. view site
mit-media-fluid-interfaces-6 | GlassProv Improv Comedy System | http://directorsfellows.media.mit.edu/google-glass-improv-google-glassprov | description:Pattie Maes, Scott Greenwald, Baratunde Thurston and Cultivated Wit As part of a Google-sponsored Glass developer event, we created a Glass-enabled improv comedy show together with noted comedians from ImprovBoston and Big Bang Improv. The actors, all wearing Glass, received cues in real time in the course of their improvisation. In contrast with the traditional model for improv comedy—punctuated by "freezing" and audience members shouting suggestions—using Glass allowed actors to seamlessly integrate audience suggestions. Actors and audience members agreed that this was a fresh take on improv comedy. It was a powerful demonstration that cues on Glass are suitable for performance: actors could become aware of the cues without having their concentration or flow interrupted, and then view them at an appropriate time thereafter. view site
mit-media-fluid-interfaces-7 | HandsOn: A Gestural System for Remote Collaboration Using Augmented Reality | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-fluid-interfaces-7 HandsOn: A Gestural System for Remote Collaboration Using Augmented Reality | description:Special Interest group(s):  Center for Terrestrial Sensing Kevin Wong and Pattie Maes 2D screens, even stereoscopic ones, limit our ability to interact with and collaborate on 3D data. We believe that an augmented reality solution, where 3D data is seamlessly integrated in the real world, is promising. We are exploring a collaborative augmented reality system for visualizing and manipulating 3D data using a head-mounted, see-through display, that allows for communication and data manipulation using simple hand gestures.
mit-media-fluid-interfaces-8 | HRQR | http://hrqr.org | description:Valentin Heun The HRQR is a visual Human and Machine Readable Quick Response Code that can replace usual 2D barcode and QR Code applications. The code can be read by humans in the same way it can be read by machines. Instead of relying on a computational error correction, the system allows a human to read the message and therefore is able to reinterpret errors in the visual image. The idea is highly inspired by a 2,000 year-old Arabic calligraphy called Kufic. view site
mit-media-fluid-interfaces-9 | Hybrid Objects | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-fluid-interfaces-9 Hybrid Objects | description:Pattie Maes and Valentin Heun A web technology-based update of Smarter Objects and the reality editor project.
mit-media-fluid-interfaces-10 | JaJan!: Remote Language Learning in Shared Virtual Space | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-fluid-interfaces-10 JaJan!: Remote Language Learning in Shared Virtual Space | description:Kevin Wong, Takako Aikawa and Pattie Maes JaJan! is a telepresence system wherein remote users can learn a second language together while sharing the same virtual environment. JaJan! can support five aspects of language learning: learning in context; personalization of learning materials; learning with cultural information; enacting language-learning scenarios; and supporting creativity and collaboration. Although JaJan! is still in an early stage, we are confident that it will bring profound changes to the ways in which we experience language learning and can make a great contribution to the field of second language education.
mit-media-fluid-interfaces-11 | LuminAR | http://fluid.media.mit.edu/people/natan/current/luminar.html | description:Natan Linder, Pattie Maes and Rony Kubat LuminAR reinvents the traditional incandescent bulb and desk lamp, evolving them into a new category of robotic, digital information devices. The LuminAR Bulb combines a Pico-projector, camera, and wireless computer in a compact form factor. This self-contained system enables users with just-in-time projected information and a gestural user interface, and it can be screwed into standard light fixtures everywhere. The LuminAR Lamp is an articulated robotic arm, designed to interface with the LuminAR Bulb. Both LuminAR form factors dynamically augment their environments with media and information, while seamlessly connecting with laptops, mobile phones, and other electronic devices. LuminAR transforms surfaces and objects into interactive spaces that blend digital media and information with the physical space. The project radically rethinks the design of traditional lighting objects, and explores how we can endow them with novel augmented-reality interfaces. view site
mit-media-fluid-interfaces-12 | MARS: Manufacturing Augmented Reality System | http://fluid.media.mit.edu/projects/mars | description:Rony Daniel Kubat, Natan Linder, Ben Weissmann, Niaja Farve, Yihui Saw and Pattie Maes Projected augmented reality in the manufacturing plant can increase worker productivity, reduce errors, gamify the workspace to increase worker satisfaction, and collect detailed metrics. We have built new LuminAR hardware customized for the needs of the manufacturing plant and software for a specific manufacturing use case. view site
mit-media-fluid-interfaces-13 | Move Your Glass | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-fluid-interfaces-13 Move Your Glass | description:Special Interest group(s):  Advancing Wellbeing Niaja Farve and Pattie Maes Move Your Glass is an activity and behavior tracker that also tries to increase wellness by nudging the wearer to engage in positive behaviors.
mit-media-fluid-interfaces-14 | Open Hybrid | http://openhybrid.org | description:Valentin Heun, Shunichi Kasahara, James Hobin, Kevin Wong, Michelle Suh, Benjamin F Reynolds, Marc Teyssier, Eva Stern-Rodriguez, Afika A Nyati, Kenny Friedman, Anissa Talantikite, Andrew Mendez, Jessica Laughlin, Pattie Maes Open Hybrid is an ppen source augmented reality platform for physical computing and Internet of Things. It is based on Web and Arduino. view site
mit-media-fluid-interfaces-15 | Reality Editor: Programming Smarter Objects | http://fluid.media.mit.edu/node/227 | description:Valentin Heun, James Hobin, Pattie Maes The Reality Editor system supports editing the behavior and interfaces of so-called "smart objects": objects or devices that have an embedded processor and communication capability. Using augmented reality techniques, the Reality Editor maps graphical elements directly on top of the tangible interfaces found on physical objects, such as push buttons or knobs. The Reality Editor allows flexible reprogramming of the interfaces and behavior of the objects, as well as defining relationships between smart objects in order to easily create new functionalities. view site
mit-media-fluid-interfaces-16 | Scanner Grabber | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-fluid-interfaces-16 Scanner Grabber | description:Tal Achituv Scanner Grabber is a digital police scanner that enables reporters to record, playback, and export audio, as well as archive public safety radio (scanner) conversations. Like a TiVo for scanners, it's an update on technology that has been stuck in the last century. It’s a great tool for newsrooms. For instance, a problem for reporters is missing the beginning of an important police incident because they have stepped away from their desk at the wrong time. Scanner Grabber solves this because conversations can be played back. Also, snippets of exciting audio, for instance a police chase, can be exported and embedded online. Reporters can listen to files while writing stories, or listen to older conversations to get a more nuanced grasp of police practices or long-term trouble spots. Editors and reporters can use the tool for collaborating, or crowdsourcing/public collaboration.
mit-media-fluid-interfaces-17 | ScreenSpire | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-fluid-interfaces-17 ScreenSpire | description:Special Interest group(s):  Advancing Wellbeing Pattie Maes, Tal Achituv, Chang Long Zhu Jin and Isa Sobrinho Screen interactions have been shown to contribute to increases in stress, anxiety, and deficiencies in breathing patterns. Since better respiration patterns can have a positive impact on wellbeing, ScreenSpire improves respiration patterns during information work using subliminal biofeedback. By using subtle graphical variations that are tuned to attempt to influence the user subconsciously user distraction and cognitive load are minimized. To enable a true seamless interaction we have adapted an RF based sensor (ResMed S+ sleep sensor) to serve as a screen-mounted contact-free and respiration sensor. Traditionally, respiration sensing is achieved with either invasive or on-skin sensors (such as a chest belt), having a contact-free sensor contributes to increased ease, comfort, and user compliance, since no special actions are required from the user.
mit-media-fluid-interfaces-18 | ShowMe: Immersive Remote Collaboration System with 3D Hand Gestures | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-fluid-interfaces-18 ShowMe: Immersive Remote Collaboration System with 3D Hand Gestures | description:Special Interest group(s):  Center for Terrestrial Sensing Pattie Maes, Judith Amores Fernandez and Xavier Benavides Palos ShowMe is an immersive mobile collaboration system that allows remote users to communicate with peers using video, audio, and gestures. With this research, we explore the use of head-mounted displays and depth sensor cameras to create a system that (1) enables remote users to be immersed in another person's view, and (2) offer a new way of sending and receiving the guidance of an expert through 3D hand gestures. With our system, both users are surrounded in the same physical environment and can perceive real-time inputs from each other.
mit-media-fluid-interfaces-19 | SmileCatcher | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-fluid-interfaces-19 SmileCatcher | description:Special Interest group(s):  Advancing Wellbeing Niaja Farve and Pattie Maes SmileCatcher is a game to be played solely or in groups that attempts to increase happiness. Research has shown that smiling correlates directly to happiness and can even produce happiness in a person. A user playing the game tries to collect as many smiles as they can from real people they interact with throughout the day. In single-player mode the user compares scores over multiple days, while multiple players compare their scores to one another. The objective of the tool is to encourage positive social interactions through gamification.
mit-media-fluid-interfaces-20 | Social Textiles | http://fluid.media.mit.edu/social-textiles | description:Hiroshi Ishii, Pattie Maes, Judith Amores Fernandez, Katsuya Fujii, Viirj Kan and Chang Long Zhu Jin Social Textiles embodies who you are and dynamically reflects your shared interests with people nearby. It enables you to gain access to communities of people in the physical world and enhances social affordances and icebreaking interactions through wearable social messaging. Social Textiles can serve to connect community members with niche interests, philosophical beliefs, personalities, emotional statuses, and ethical views. It has the potential to enable members to bypass superficial or generic interests through “filtering” individuals, to tune social experiences toward people who are more compatible. view site
mit-media-fluid-interfaces-21 | STEM Accessibility Tool | http://fluid.media.mit.edu/node/327 | description:Pattie Maes and Rahul Namdev We are developing a very intuitive and interactive platform to make complex information–especially science, technology, engineering, and mathematics (STEM) material–truly accessible to blind and visually impaired students by using a tactile device with no loss of information compared with printed materials. A key goal of this project is to develop tactile information-mapping protocols through which the tactile interface can best convey educational and other graphical materials. view site
mit-media-fluid-interfaces-22 | TagMe | http://fluid.media.mit.edu/projects/tagme | description:Pattie Maes, Judith Amores Fernandez and Xavier Benavides Palos TagMe is an end-user toolkit for easy creation of responsive objects and environments. It consists of a wearable device that recognizes the object or surface the user is touching. The user can make everyday objects come to life through the use of RFID tag stickers, which are read by an RFID bracelet whenever the user touches the object. We present a novel approach to create simple and customizable rules based on emotional attachment to objects and social interactions of people. Using this simple technology, the user can extend their application interfaces to include physical objects and surfaces into their personal environment, allowing people to communicate through everyday objects in very low-effort ways. view site
mit-media-fluid-interfaces-23 | The Challenge | http://challenge.media.mit.edu | description:Special Interest group(s):  Advancing Wellbeing Natasha Jaques, Niaja Farve, Pattie Maes and Rosalind W. Picard Individuals who work in sedentary occupations are at increased risk of a number of serious health consequences. This project involves both a tool and an experiment aimed at decreasing sedentary activity and promoting social connections among members of the MIT Media Lab. Our system will ask participants to sign up for short physical challenges (ping pong, foosball, walking) and pair them with a partner to perform the activity. Participants' overall activity levels will be monitored with an activity tracker during the course of the study to assess the effectiveness of the system. view site
