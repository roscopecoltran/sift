COMPGI13-I |THE PROBLEM |  | parentid:COMPGI13
COMPGI13-1 |Introduction |  | parentid:COMPGI13
COMPGI13-1.1 |Reinforcement Learning |  | parentid:COMPGI13-1
COMPGI13-1.3 |Elements of Reinforcement Learning |  | parentid:COMPGI13-1
COMPGI13-1.4 |An Extended Example: Tic-Tac-Toe |  | parentid:COMPGI13-1
COMPGI13-1.6 |History of Reinforcement Learning |  | parentid:COMPGI13-1
COMPGI13-1.7 |Bibliographical Remarks |  | parentid:COMPGI13-1
COMPGI13-2 |Evaluative Feedback |  | parentid:COMPGI13
COMPGI13-2.1 |AnÂ n-Armed Bandit Problem |  | parentid:COMPGI13-2
COMPGI13-2.2 |Action-Value Methods |  | parentid:COMPGI13-2
COMPGI13-2.3 |Softmax Action Selection |  | parentid:COMPGI13-2
COMPGI13-2.4 |Evaluation Versus Instruction |  | parentid:COMPGI13-2
COMPGI13-2.5 |Incremental Implementation |  | parentid:COMPGI13-2
COMPGI13-2.6 |Tracking a Nonstationary Problem |  | parentid:COMPGI13-2
COMPGI13-2.7 |Optimistic Initial Values |  | parentid:COMPGI13-2
COMPGI13-2.8 |Reinforcement Comparison |  | parentid:COMPGI13-2
COMPGI13-2.9 |Pursuit Methods |  | parentid:COMPGI13-2
COMPGI13-2.10 |Associative Search |  | parentid:COMPGI13-2
COMPGI13-2.12 |Bibliographical and Historical Remarks |  | parentid:COMPGI13-2
COMPGI13-3 |The Reinforcement Learning Problem |  | parentid:COMPGI13
COMPGI13-3.1 |The Agent-Environment Interface |  | parentid:COMPGI13-3
COMPGI13-3.2 |Goals and Rewards |  | parentid:COMPGI13-3
COMPGI13-3.5 |The Markov Property |  | parentid:COMPGI13-3
COMPGI13-3.6 |Markov Decision Processes |  | parentid:COMPGI13-3
COMPGI13-3.7 |Value Functions |  | parentid:COMPGI13-3
COMPGI13-3.8 |Optimal Value Functions |  | parentid:COMPGI13-3
COMPGI13-3.9 |Optimality and Approximation |  | parentid:COMPGI13-3
COMPGI13-3.11 |Bibliographical and Historical Remarks |  | parentid:COMPGI13-3
COMPGI13-II |ELEMENTARY SOLUTION METHODS |  | parentid:COMPGI13
COMPGI13-4 |Dynamic Programming |  | parentid:COMPGI13
COMPGI13-4.1 |Policy Evaluation |  | parentid:COMPGI13-4
COMPGI13-4.2 |Policy Improvement |  | parentid:COMPGI13-4
COMPGI13-4.3 |Policy Iteration |  | parentid:COMPGI13-4
COMPGI13-4.4 |Value Iteration |  | parentid:COMPGI13-4
COMPGI13-4.5 |Asynchronous Dynamic Programming |  | parentid:COMPGI13-4
COMPGI13-4.6 |Generalized Policy Iteration |  | parentid:COMPGI13-4
COMPGI13-4.7 |Efficiency of Dynamic Programming |  | parentid:COMPGI13-4
COMPGI13-4.9 |Bibliographical and Historical Remarks |  | parentid:COMPGI13-4
COMPGI13-5 |Monte Carlo Methods |  | parentid:COMPGI13
COMPGI13-5.1 |Monte Carlo Policy Evaluation |  | parentid:COMPGI13-5
COMPGI13-5.2 |Monte Carlo Estimation of Action Values |  | parentid:COMPGI13-5
COMPGI13-5.3 |Monte Carlo Control |  | parentid:COMPGI13-5
COMPGI13-5.4 |On-Policy Monte Carlo Control |  | parentid:COMPGI13-5
COMPGI13-5.5 |Evaluating One Policy While Following Another |  | parentid:COMPGI13-5
COMPGI13-5.6 |Off-Policy Monte Carlo Control |  | parentid:COMPGI13-5
COMPGI13-5.7 |Incremental Implementation |  | parentid:COMPGI13-5
COMPGI13-5.9 |Bibliographical and Historical Remarks |  | parentid:COMPGI13-5
COMPGI13-6 |Temporal-Difference Learning |  | parentid:COMPGI13
COMPGI13-6.1 |TD Prediction |  | parentid:COMPGI13-6
COMPGI13-6.2 |Advantages of TD Prediction Methods |  | parentid:COMPGI13-6
COMPGI13-6.3 |Optimality of TD(O) |  | parentid:COMPGI13-6
COMPGI13-6.4 |Sarsa: On-Policy TD Control |  | parentid:COMPGI13-6
COMPGI13-6.5 |Q-Learning: Off-Policy TD Control |  | parentid:COMPGI13-6
COMPGI13-6.6 |Actor-Critic Methods |  | parentid:COMPGI13-6
COMPGI13-6.7 |R-Learning for Undiscounted Continuing Tasks |  | parentid:COMPGI13-6
COMPGI13-6.8 |Games, Afterstates, and Other Special Cases |  | parentid:COMPGI13-6
COMPGI13-6.10 |Bibliographical and Historical Remarks |  | parentid:COMPGI13-6
COMPGI13-III |A UNIFIED VIEW |  | parentid:COMPGI13
COMPGI13-7 |Eligibility Traces |  | parentid:COMPGI13
COMPGI13-7.1 |n-Step TD Prediction |  | parentid:COMPGI13-7
COMPGI13-7.2 |The Forward View of TD(l) |  | parentid:COMPGI13-7
COMPGI13-7.3 |The Backward View of TD(l) |  | parentid:COMPGI13-7
COMPGI13-7.4 |Equivalence of Forward and Backward Views |  | parentid:COMPGI13-7
COMPGI13-7.7 |Eligibility Traces for Actor-Critic Methods |  | parentid:COMPGI13-7
COMPGI13-7.8 |Replacing Traces |  | parentid:COMPGI13-7
COMPGI13-7.9 |Implementation Issues |  | parentid:COMPGI13-7
COMPGI13-7.12 |Bibliographical and Historical Remarks |  | parentid:COMPGI13-7
COMPGI13-8 |Generalization and Function Approximation |  | parentid:COMPGI13
COMPGI13-8.1 |Value Prediction with Function Approximation |  | parentid:COMPGI13-8
COMPGI13-8.2 |Gradient-Descent Methods |  | parentid:COMPGI13-8
COMPGI13-8.3 |Linear Methods |  | parentid:COMPGI13-8
COMPGI13-8.4 |Control with Function Approximation |  | parentid:COMPGI13-8
COMPGI13-8.5 |Off-Policy Bootstrapping |  | parentid:COMPGI13-8
COMPGI13-8.6 |Should We Bootstrap? |  | parentid:COMPGI13-8
COMPGI13-8.8 |Bibliographical and Historical Remarks |  | parentid:COMPGI13-8
COMPGI13-9 |Planning and Learning |  | parentid:COMPGI13
COMPGI13-9.1 |Models and Planning |  | parentid:COMPGI13-9
COMPGI13-9.2 |Integrating Planning, Acting, and Learning |  | parentid:COMPGI13-9
COMPGI13-9.3 |When the Model Is Wrong |  | parentid:COMPGI13-9
COMPGI13-9.4 |Prioritized Sweeping |  | parentid:COMPGI13-9
COMPGI13-9.5 |Full vs. Sample Backups |  | parentid:COMPGI13-9
COMPGI13-9.6 |Trajectory Sampling |  | parentid:COMPGI13-9
COMPGI13-9.7 |Heuristic Search |  | parentid:COMPGI13-9
COMPGI13-9.9 |Bibliographical and Historical Remarks |  | parentid:COMPGI13-9
COMPGI13-10 |Dimensions of Reinforcement Learning |  | parentid:COMPGI13
COMPGI13-10.1 |The Unified View |  | parentid:COMPGI13-10
COMPGI13-10.2 |Other Frontier Dimensions |  | parentid:COMPGI13-10
COMPGI13-11 |Case Studies |  | parentid:COMPGI13
COMPGI13-11.2 |Samuel s Checkers Player |  | parentid:COMPGI13-11
COMPGI13-11.3 |The Acrobot |  | parentid:COMPGI13-11
COMPGI13-11.4 |Elevator Dispatching |  | parentid:COMPGI13-11
COMPGI13-11.5 |Dynamic Channel Allocation |  | parentid:COMPGI13-11
COMPGI13-11.6 |Job-Shop Scheduling |  | parentid:COMPGI13-11
COMPGI13-SUMMARY |OF NOTATION |  | parentid:COMPGI13
