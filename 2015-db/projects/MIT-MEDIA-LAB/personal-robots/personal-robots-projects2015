mit-media-personal-robots-1 | AIDA: Affective Intelligent Driving Agent | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-personal-robots-1 AIDA: Affective Intelligent Driving Agent | description:Cynthia Breazeal and Kenton Williams Drivers spend a significant amount of time multi-tasking while they are behind the wheel. These dangerous behaviors, particularly texting while driving, can lead to distractions and ultimately to accidents. Many in-car interfaces designed to address this issue still neither take a proactive role to assist the driver nor leverage aspects of the driver's daily life to make the driving experience more seamless. In collaboration with Volkswagen/Audi and the SENSEable City Lab, we are developing AIDA (Affective Intelligent Driving Agent), a robotic driver-vehicle interface that acts as a sociable partner. AIDA elicits facial expressions and strong non-verbal cues for engaging social interaction with the driver. AIDA also leverages the driver's mobile device as its face, which promotes safety, offers proactive driver support, and fosters deeper personalization to the driver.
mit-media-personal-robots-2 | Animal-Robot Interaction | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-personal-robots-2 Animal-Robot Interaction | description:Brad Knox, Patrick Mccabe and Cynthia Breazeal Like people, dogs and cats live among technologies that affect their lives. Yet little of this technology has been designed with pets in mind. We are developing systems that interact intelligently with animals to entertain, exercise, and empower them. Currently, we are developing a laser-chasing game, in which dogs or cats are tracked by a ceiling-mounted webcam, and a computer-controlled laser is moved with knowledge of the pet's position and movement. Machine learning will be applied to optimize the specific laser strategy. We envision enabling owners to initiate and view the interaction remotely through a web interface, providing stimulation and exercise to pets when the owners are at work or otherwise cannot be present.
mit-media-personal-robots-3 | Cloud-HRI | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-personal-robots-3 Cloud-HRI | description:Cynthia Breazeal, Nicholas DePalma, Adam Setapen and Sonia Chernova Imagine opening your eyes and being awake for only half an hour at a time. This is the life that robots traditionally live. This is due to a number of factors such as battery life and wear on prototype joints. Roboticists have typically muddled though this challenge by crafting handmade perception and planning models of the world, or by using machine learning with synthetic and real-world data, but cloud-based robotics aims to marry large distributed systems with machine learning techniques to understand how to build robots that interpret the world in a richer way. This movement aims to build large-scale machine learning algorithms that use experiences from large groups of people, whether sourced from a large number of tabletop robots or a large number of experiences with virtual agents. Large-scale robotics aims to change embodied AI as it changed non-embodied AI.
mit-media-personal-robots-4 | Command Not Found | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-personal-robots-4 Command Not Found | description:David Nunez, Tod Machover, Cynthia Breazeal A performance between a human and a robot tells the story of growing older and trying to maintain friendships with those we meet along the way. This project explores live-coding with a robot, in which the actor creates and executes software on a robot in real time; the audience can watch the program evolve on screen and the code, itself, is part of the narrative.
mit-media-personal-robots-5 | DragonBot: Android Phone Robots for Long-Term HRI | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-personal-robots-5 DragonBot: Android Phone Robots for Long-Term HRI | description:Adam Setapen, Natalie Freed, and Cynthia Breazeal DragonBot is a new platform built to support long-term interactions between children and robots. The robot runs entirely on an Android cell phone, which displays an animated virtual face. Additionally, the phone provides sensory input (camera and microphone) and fully controls the actuation of the robot (motors and speakers). Most importantly, the phone always has an Internet connection, so a robot can harness cloud-computing paradigms to learn from the collective interactions of multiple robots. To support long-term interactions, DragonBot is a "blended-reality" character—if you remove the phone from the robot, a virtual avatar appears on the screen and the user can still interact with the virtual character on the go. Costing less than $1,000, DragonBot was specifically designed to be a low-cost platform that can support longitudinal human-robot interactions "in the wild."
mit-media-personal-robots-6 | Global Literacy Tablets | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-personal-robots-6 Global Literacy Tablets | description:Cynthia Breazeal, David Nunez, Tinsley Galyean, Maryanne Wolf (Tufts), and Robin Morris (GSU) We are developing a system of early literacy apps, games, toys, and robots that will triage how children are learning, diagnose literacy deficits, and deploy dosages of content to encourage app play using a mentoring algorithm that recommends an appropriate activity given a child's progress. Currently, over 200 Android-based tablets have been sent to children around the world; these devices are instrumented to provide a very detailed picture of how kids are using these technologies. We are using this big data to discover usage and learning models that will inform future educational development.
mit-media-personal-robots-7 | Huggable: A Social Robot for Pediatric Care | http://robotic.media.mit.edu/portfolio/pediatric-companion/ | description:Special Interest group(s):  Advancing Wellbeing Boston Children's Hospital, Northeastern University, Cynthia Breazeal, Sooyeon Jeong, Fardad Faridi and Jetta Company Children and their parents may undergo challenging experiences when admitted for inpatient care at pediatric hospitals. While most hospitals make efforts to provide socio-emotional support for patients and their families during care, gaps still exist between human resource supply and demand. The Huggable project aims to close this gap by creating a social robot able to mitigate stress, anxiety, and pain in pediatric patients by engaging them in playful interactions. In collaboration with Boston Children’s Hospital and Northeastern University, we are currently running an experimental study to compare the effects of the Huggable robot to a virtual character on a screen and a plush teddy bear. We demonstrated preliminarily that children are more eager to emotionally connect with and be physically activated by a robot than a virtual character, illustrating the potential of social robots to provide socio-emotional support during inpatient pediatric care. view site
mit-media-personal-robots-8 | Mind-Theoretic Planning for Robots | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-personal-robots-8 Mind-Theoretic Planning for Robots | description:Cynthia Breazeal and Sigurdur Orn Adalgeirsson Mind-Theoretic Planning (MTP) is a technique for robots to plan in social domains. This system takes into account probability distributions over the initial beliefs and goals of people in the environment that are relevant to the task, and creates a prediction of how they will rationally act on their beliefs to achieve their goals. The MTP system then proceeds to create an action plan for the robot that simultaneously takes advantage of the effects of anticipated actions of others and also avoids interfering with them.
mit-media-personal-robots-9 | Robot Learning from Human-Generated Rewards | http://www.bradknox.net/human-reward/ | description:Brad Knox, Robert Radway, Tom Walsh, and Cynthia Breazeal To serve us well, robots and other agents must understand our needs and how to fulfill them. To that end, our research develops robots that empower humans by interactively learning from them. Interactive learning methods enable technically unskilled end-users to designate correct behavior and communicate their task knowledge to improve a robot's task performance. This research on interactive learning focuses on algorithms that facilitate teaching by signals of approval and disapproval from a live human trainer. We operationalize these feedback signals as numeric rewards within the machine-learning framework of reinforcement learning. In comparison to the complementary form of teaching by demonstration, this feedback-based teaching may require less task expertise and place less cognitive load on the trainer. Envisioned applications include human-robot collaboration and assistive robotic devices for handicapped users, such as myolectrically controlled prosthetics. view site
mit-media-personal-robots-10 | Robotic Language Learning Companions | http://robotic.media.mit.edu/portfolio/cyberlearning/ | description:Cynthia Breazeal, Jacqueline Kory Westlund, Sooyeon Jeong, Paul Harris, Dave DeSteno, and Leah Dickens Young children learn language not through listening alone, but through active communication with a social actor. Cultural immersion and context are also key in long-term language development. We are developing robotic conversational partners and hybrid physical/digital environments for language learning. For example, the robot Sophie helped young children learn French through a food-sharing game. The game was situated on a digital tablet embedded in a café table. Sophie modeled how to order food and as the child practiced the new vocabulary, the food was delivered via digital assets onto the table's surface. A teacher or parent can observe and shape the interaction remotely via a digital tablet interface to adjust the robot's conversation and behavior to support the learner. More recently, we have been examining how social nonverbal behaviors impact children's perceptions of the robot as an informant and social companion. view site
mit-media-personal-robots-11 | Robotic Learning Companions | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-personal-robots-11 Robotic Learning Companions | description:Cynthia Breazeal, Jacqueline Kory Westlund, and Samuel Spaulding The language and literacy skills of young children entering school are highly predictive of their long-term academic success. Children from low-income families are particularly at risk. Parents often work multiple jobs, giving them less time to talk to and read with their children. The parents might be illiterate or not speak the language taught in local schools, and these parents may not have been read to as children, providing less experience of good co-reading practice to draw upon. We are currently developing a robotic reading companion for young children, trained by interactive demonstrations from parents and/or educational experts. We intend for this robot to complement parental interaction and emulate some of their best practices in co-reading, building language and literacy through asking comprehension questions, prompting exploration, and simply being emotionally involved in the child’s reading experience.
mit-media-personal-robots-12 | Socially Assistive Robotics: An NSF Expedition in Computing | http://www.robotshelpingkids.org/people.php | description:Tufts University, University of Southern California, Kasia Hayden with Stanford University, Cynthia Breazeal, Edith Ackermann, Goren Gordon, Michal Gordon, Sooyeon Jeong, Jacqueline Kory, Jin Joo Lee, Luke Plummer, Samuel Spaulding, Willow Garage and Yale Our mission is to develop the computational techniques that will enable the design, implementation, and evaluation of "relational" robots, in order to encourage social, emotional, and cognitive growth in children, including those with social or cognitive deficits. Funding for the project comes from the NSF Expeditions in Computing program. This expedition has the potential to substantially impact the effectiveness of education and healthcare, and to enhance the lives of children and other groups that require specialized support and intervention. In particular, the MIT effort is focusing on developing second-language learning companions for pre-school aged children, ultimately for ESL (English as a Second Language). view site
mit-media-personal-robots-13 | Tega: A New Robot Platform for Long-Term Interaction | http://robotic.media.mit.edu/portfolio/tega/ | description:Cooper Perkins Inc., Fardad Faridi, Cynthia Breazeal, Jin Joo Lee, Luke Plummer, IFRobots and Stacey Dyer Tega is a new robot platform for long-term interactions with children. The robot leverages smart phones to graphically display facial expressions. Smart phones are also used for computational needs including behavioral control, sensor processing, and motor control to drive its five degrees of freedom. To withstand long-term continual use, we have designed an efficient battery-powered system that can potentially run for up to six hours before needing to be charged. We also designed for more robust and reliable actuator movements so that the robot can express consistent and expressive behaviors over long periods of time. Through its small size and furry exterior, the robot is aesthetically designed for children. We aim to field test the robot's ability to work reliably in out-of-lab environments and engage young children in educational activities. view site
mit-media-personal-robots-14 | TinkRBook: Reinventing the Reading Primer | https://www.google.com.hk/?gws_rd=cr,ssl#safe=strict&q=mit-media-personal-robots-14 TinkRBook: Reinventing the Reading Primer | description:Cynthia Breazeal, Angela Chang, and David Nunez TinkRBook is a storytelling system that introduces a new concept of reading, called textual tinkerability. Textual tinkerability uses storytelling gestures to expose the text-concept relationships within a scene. Tinkerability prompts readers to become more physically active and expressive as they explore concepts in reading together. TinkRBooks are interactive storybooks that prompt interactivity in a subtle way, enhancing communication between parents and children during shared picture-book reading. TinkRBooks encourage positive reading behaviors in emergent literacy: parents act out the story to control the words onscreen, demonstrating print referencing and dialogic questioning techniques. Young children actively explore the abstract relationship between printed words and their meanings, even before this relationship is properly understood. By making story elements alterable within a narrative, readers can learn to read by playing with how word choices impact the storytelling experience. Recently, this research has been applied in developing countries.
