MLG10401 | Introduction to Machine Learning (Undergrad) | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10401&SEMESTER=S17 | instructors:Poczos, Barnabas prereq:(15122) and (15151 or 21127 or 21128) and (36217 or 36225 or 21325 or 15359) description:Machine learning is subfield of computer science with the goal of exploring, studying, and developing learning systems, methods, and algorithms that can improve their performance with learning from data. This course is designed to give undergraduate students a one-semester-long introduction to the main principles, algorithms, and applications of machine learning.   Topics. The topics of this course will be in part parallel with those covered in the graduate machine learning courses (10-715, 10-701, 10-601), but with a greater emphasis on applications and case studies in machine learning.  After completing the course, students will be able to:  *select and apply an appropriate supervised learning algorithm for classification problems (e.g., naive Bayes, perceptron, support vector machine, logistic regression).   *select and apply an appropriate supervised learning algorithm for regression problems (e.g., linear regression, ridge regression).  *recognize different types of unsupervised learning problems, and select and apply appropriate algorithms (e.g., clustering, linear and nonlinear dimensionality reduction).   *work with probabilities (Bayes rule, conditioning, expectations, independence), linear algebra (vector and matrix operations, eigenvectors, SVD), and calculus (gradients, Jacobians) to derive machine learning methods such as linear regression, naive Bayes, and principal components analysis.   *understand machine learning principles such as model selection, overfitting, and underfitting, and techniques such as cross-validation and regularization.   *implement machine learning algorithms such as logistic regression via stochastic gradient descent, linear regression (using a linear algebra toolbox), perceptron, or k-means clustering.   *run appropriate supervised and unsupervised learning algorithms on real and synthetic data sets and interpret the results.
MLG10500 | Senior Research Project | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10500&SEMESTER=S17 | instructors:Cohen, William description:Register for this course if you are minoring in Machine Learning. This course is intended for research with a faculty member that would count towards the minor.
MLG10600 | Mathematical background for Machine Learning | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10600&SEMESTER=F16 | instructors:Gordon, Geoffrey description:This course provides a place for students to practice the necessary mathematical background for further study in machine learning -- particularly for taking 10-601 and 10-701. Topics covered include probability, linear algebra (inner product spaces, linear operators), multivariate differential calculus, optimization, and likelihood functions. The course assumes some background in each of the above, but will review and give practice in each. Some coding will be required: the course will provide practice with translating the above mathematical concepts into concrete programs. The course is split into two minis, which form a sequence (A1 is a prerequisite for A2).
MLG10601 | Introduction to Machine Learning (Masters) | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10601&SEMESTER=S17 | instructors:Gormley, Matthew prereq:(15122) and (21127 or 15151) and (36217 or 36225 or 15359 or 21325) description:Machine Learning (ML) develops computer programs that automatically improve their performance through experience. This includes learning many types of tasks based on many types of experience, e.g. spotting high-risk medical patients, recognizing speech, classifying text documents, detecting credit card fraud, or driving autonomous vehicles. 10601 covers all or most of: concept learning, decision trees, neural networks, linear learning, active learning, estimation  the bias-variance tradeoff, hypothesis testing, Bayesian learning, the MDL principle, the Gibbs classifier, Naive Bayes, Bayes Nets  Graphical Models, the EM algorithm, Hidden Markov Models, K-Nearest-Neighbors and nonparametric learning, reinforcement learning, bagging, boosting and discriminative training. Grading will be based on weekly or biweekly assignments (written and/or programming), a midterm, a final exam. 10601 is recommended for CS Seniors  Juniors, quantitative Masters students,  non-MLD PhD students. Prerequisites (strictly enforced): strong quantitative aptitude, college probability  statistics course, and programming proficiency. For learning to apply ML practically  effectively, without the above prerequisites, consider 11344/05834 instead. You can evaluate your ability to take the course via a self-assessment exam (http://bit.ly/2fkddDN). Also, be sure to read the ML course comparison (http://bit.ly/2eV3UaD).
MLG10605 | Machine Learning with Large Datasets | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10605&SEMESTER=F16 | instructors:Cohen, William prereq:15210 or 15214 description:Large datasets are difficult to work with for several reasons. They are difficult to visualize, and it is difficult to understand what sort of errors and biases are present in them. They are computationally expensive to process, and often the cost of learning is hard to predict - for instance, and algorithm that runs quickly in a dataset that fits in memory may be exorbitantly expensive when the dataset is too large for memory. Large datasets may also display qualitatively different behavior in terms of which learning methods produce the most accurate predictions.  This course is intended to provide a student practical knowledge of, and experience with, the issues involving large datasets. Among the issues considered are: scalable learning techniques, such as streaming machine learning techniques; parallel infrastructures such as map-reduce; practical techniques for reducing the memory requirements for learning methods, such as feature hashing and Bloom filters; and techniques for analysis of programs in terms of memory, disk usage, and (for parallel methods) communication complexity.  The class will include programming assignments, and a one-month short project chosen by the student. The project will be designed to compare the scalability of variant learning algorithms on datasets.  An introductory course in machine learning, like 10-601 or 10-701, is a prerequisite or a co-requisite. If you plan to take this course and 10-601 concurrently please tell the instructor.  The course will include several substantial programming assignments, so an additional prerequisite is 15-211, or 15-214, or comparable familiarity with Java and good programming skills.  Undergraduates need permission of the instructor to enroll.
MLG10611 | MS DAP Research | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10611&SEMESTER=S17 | instructors:Cohen, William description:This course is for ML Masters students to work on research with their advisor for the Data Analysis Project.
MLG10620 | Independent Study: Research | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10620&SEMESTER=S17 | instructors:Cohen, William description:Independent Study intended to work on research with a Machine Learning faculty member.
MLG10697 | Reading and Research | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10697&SEMESTER=S17 | instructors:Cohen, William description:Course for MS students to work with their advisor on research
MLG10701 | Introduction to Machine Learning (PhD) | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10701&SEMESTER=S17 | instructors:Singh, Aarti Ravikumar, Pradeep prereq:(15122) and (15151 or 21127) and (15359 or 36225 or 21325 or 36217) description:Machine learning studies the question How can we build computer programs that automatically improve their performance through experience?   This includes learning to perform many types of tasks based on many types of experience.  For example, it includes robots learning to better navigate based on experience gained by roaming their environments, medical decision aids that learn to predict which therapies work best for which diseases based on data mining of historical health records, and speech recognition systems that learn to better understand your speech based on experience listening to you.  This course is designed to give PhD students a thorough grounding in the methods, mathematics and algorithms needed to do research and applications in machine learning. Students entering the class with a pre-existing working knowledge of probability, statistics and algorithms will be at an advantage, but the class has been designed so that anyone with a strong numerate background can catch up and fully participate. You can evaluate your ability to take the course via a self-assessment exam that will be made available to you  after you register.  If you are interested in this topic, but are not a PhD student, or are a PhD student not specializing in machine learning, you might consider the masters level course on Machine Learning, 10-601.  This class may be appropriate for MS and undergrad students who are interested in the theory and algorithms behind ML.  You can evaluate your ability to take the course via a self-assessment exam at: https://qna-app.appspot.com/view.html?aglzfnFuYS1hcHByGQsSDFF1ZXN0aW9uTGlzdBiAgICgpO-KCgw ML course comparison: https://docs.google.com/document/d/1Y0Jx_tcINWQrWJx31WGEQSsUs059OUMmPIVSeyxNdeM/edit
MLG10702 | Statistical Machine Learning | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10702&SEMESTER=S17 | instructors:Wasserman, Larry Tibshirani, Ryan prereq:(10705 or 36705) and (10701 or 10715) description:Statistical Machine Learning is a second graduate level course in advanced machine learning, assuming that students have taken Machine Learning (10-701) or Advanced Machine Learning (10-715), and Intermediate Statistics (36-705). The term ?statistical? in the title reflects the emphasis on statistical theory and methodology.  This course is mostly focused on methodology and theoretical foundations. It treats both the ?art? of designing good learning algorithms and the ?science? of analyzing an algorithm?s statistical properties and performance guarantees. Theorems are presented together with practical aspects of methodology and intuition to help students develop tools for selecting appropriate methods and approaches to problems in their own research.  Though computation is certainly a critical component of what makes a method successful, it will not receive the same central focus as methodology and theory.   We will cover topics in statistical theory that are important for researchers in machine learning, including consistency, minimax estimation, and concentration of measure. We will also cover statistical topics that may not be covered in as much depth in other machine learning courses, such as nonparametric density estimation, nonparametric regression, and Bayesian estimation.
MLG10703 | Deep Reinforcement Learning &amp; Control | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10703&SEMESTER=S17 | instructors:Salakhutdinov, Ruslan Fragkiadaki, Aikaterini description:This course will cover latest advances in Reinforcement Learning and Control, such as, deep Q learning, actor-critic methods, learning and planning, concurrent trajectory optimization and policy learning, inverse reinforcement learning, hierarchical reinforcement learning methods, forward predictive models, deep model predictive control, exploration strategies, adaptive control, applications to deep robotic learning. By the end of the course you should be able to: 1) code up a suitable reinforcement learning method in simulation or on robotic platform for a task 2) identify what are easy and hard problems in RL and learning for robotics  The course will have a final project which will involve design of a reinforcement learning method in simulation or robotic platform. The homeworks will be in OpenAI gym. Pre-requisites: Students should have a basic background in algorithms, linear algebra, machine Learning, deep learning.
MLG10704 | Information Processing and Learning | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10704&SEMESTER=F16 | instructors:Singh, Aarti description:Whats the connection between how many bits we can send over a channel and how accurately we can classify documents or fit a curve to data? Is there any link between decision trees, prefix codes and wavelet transforms? What about max-entropy and maximum likelihood, or universal coding and online learning? This inter-disciplinary course will explore these and other questions that link the fields of information theory, signal processing, and machine learning, all of which aim to understand the information contained in data. The goal is to highlight the common concepts and establish concrete links between these fields that enable efficient information processing and learning.  The course will do a short but introductory review of basic information theory, including entropy and fundamental limits of data compression, data processing and Fanos inequalities, channel capacity, and rate-distortion theory. Then we will dig into the connections to learning including: estimation of information theoretic quantities (such entropy, mutual information, and divergence) and their applications in learning, information theoretic lower bounds for machine learning problems, duality of max entropy and maximum likelihood, connections between clustering and rate-distortion theory, universal coding and online learning, and more.  Pre-requisites: Fundamentals of Probability, Statistics, Linear Algebra and Real analysis.
MLG10708 | Probabilistic Graphical Models | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10708&SEMESTER=S17 | instructors:Xing, Eric prereq:10701 or 15781 description:Many of the problems in artificial intelligence, statistics, computer systems, computer vision, natural language processing, and computational biology, among many other fields, can be viewed as the search for a coherent global conclusion from local information. The probabilistic graphical models framework provides an unified view for this wide range of problems, enabling efficient inference, decision-making and learning in problems with a very large number of attributes and huge datasets. This graduate-level course will provide you with a strong foundation for both applying graphical models to complex problems and for addressing core research topics in graphical models.  The class will cover three aspects: The core representation, including Bayesian and Markov networks, and dynamic Bayesian networks; probabilistic inference algorithms, both exact and approximate; and, learning methods for both the parameters and the structure of graphical models. Students entering the class should have a pre-existing working knowledge of probability, statistics, and algorithms, though the class has been designed to allow students with a strong numerate background to catch up and fully participate.  It is expected that after taking this class, the students should have obtain sufficient working knowledge of multi-variate probabilistic modeling and inference for practical applications, should be able to formulate and solve a wide range of problems in their own domain using GM, and can advance into more specialized technical literature by themselves.  Students are required to have successfully completed 10701 or 10715, or an equivalent class.
MLG10715 | Advanced Introduction to Machine Learning | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10715&SEMESTER=F16 | instructors:Poczos, Barnabas description:The rapid improvement of sensory techniques and processor speed, and the availability of inexpensive massive digital storage, have led to a growing demand for systems that can automatically comprehend and mine massive and complex data from diverse sources.  Machine Learning is becoming the primary mechanism by which information is extracted from Big Data, and a primary pillar that Artificial Intelligence is built upon.  This course is designed for Ph.D. students whose primary field of study is machine learning, or who intend to make machine learning methodological research a main focus of their thesis.  It will give students a thorough grounding in the algorithms, mathematics, theories, and insights needed to do in-depth research and applications in machine learning. The topics of this course will in part parallel those covered in the general graduate machine learning course (10-701), but with a greater emphasis on depth in theory and algorithms.   The course will also include additional advanced topics such as privacy in machine learning, interactive learning, reinforcement learning, online learning,  Bayesian nonparametrics, and additional material on graphical models.  Students entering the class are expected to have a pre-existing strong working knowledge of algorithms, linear algebra, probability, and statistics.  If you are interested in this topic, but do not have the required background or are not planning to work on a PhD thesis with machine learning as the main focus, you might consider the general graduate Machine Learning course (10-701) or the Masters-level Machine Learning course (10-601). ML course comparison: https://docs.google.com/document/d/1Y0Jx_tcINWQrWJx31WGEQSsUs059OUMmPIVSeyxNdeM/edit
MLG10725 | Convex Optimization | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10725&SEMESTER=F16 | instructors:Tibshirani, Ryan Pena, Javier description:Nearly every problem in machine learning can be formulated as the optimization of some function, possibly under some set of constraints.  This universal reduction may seem to suggest that such optimization tasks are intractable. Fortunately, many real world problems have special structure, such as convexity, smoothness, separability, etc., which allow us to formulate optimization problems that can often be solved efficiently. This course is designed to give a graduate-level student a thorough grounding in the formulation of optimization problems that exploit such structure, and in efficient solution methods for these problems. The main focus is on the formulation and solution of convex optimization problems. These general concepts will also be illustrated through applications in machine learning and statistics.  Students entering the class should have a pre-existing working knowledge of algorithms, though the class has been designed to allow students with a strong numerate background to catch up and fully participate. Though not required, having taken 10-701 or an equivalent machine learning or statistics class is strongly encouraged, since we will use applications in machine learning and statistics to demonstrate the concepts we cover in class.  Students will work on an extensive optimization-based project throughout the semester; those wanting to take the class without the project can register under the 9 unit option.
MLG10805 | Machine Learning with Large Datasets | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10805&SEMESTER=F16 | instructors:Cohen, William description:Large datasets are difficult to work with for several reasons. They are difficult to visualize, and it is difficult to understand what sort of errors and biases are present in them. They are computationally expensive to process, and often the cost of learning is hard to predict - for instance, and algorithm that runs quickly in a dataset that fits in memory may be exorbitantly expensive when the dataset is too large for memory. Large datasets may also display qualitatively different behavior in terms of which learning methods produce the most accurate predictions.  This course is intended to provide a student practical knowledge of, and experience with, the issues involving large datasets. Among the issues considered are: scalable learning techniques, such as streaming machine learning techniques; parallel infrastructures such as map-reduce; practical techniques for reducing the memory requirements for learning methods, such as feature hashing and Bloom filters; and techniques for analysis of programs in terms of memory, disk usage, and (for parallel methods) communication complexity.  An introductory course in machine learning, like 10-601 or 10-701, is a prerequisite or a co-requisite.     The class will include programming assignments, presentation of relevant research papers to the class, and a research project chosen by the student, to be presented to the class, and written up in a conference-paper format.     10-805 will share lectures with 10-605, but 10-805 students need to make class presentations and complete a research project, and will do fewer programming assignments, so 10-805 students are expected to be capable of surveying recent literature and conducting research. Four lecture sessions for 10-605 will also be reserved for 10-805 students presentations.    If there is sufficient interest we will introduce a mechanism for 10-605 students to collaborate of 10-805 students on projects.
MLG10807 | Topics in Deep Learning | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10807&SEMESTER=F16 | instructors:Salakhutdinov, Ruslan prereq:10715 or 10701 description:Building intelligent machines that are capable of extracting meaningful representations from high-dimensional data lies at the core of solving many AI related tasks. In the past few years, researchers across many different communities, from applied statistics to engineering, computer science and neuroscience, have developed deep (hierarchical) models -- models that are composed of several layers of nonlinear processing. An important property of these models is that they can learn useful representations by re-using and combining intermediate concepts, allowing these models to be successfully applied in a wide variety of domains, including visual object recognition, information retrieval, natural language processing, and speech perception.  This is an advanced graduate course, designed for Masters and Ph.D. level students, and will assume a reasonable degree of mathematical maturity. The goal of this course is to introduce students to the recent and exciting developments of various deep learning methods. Some topics to be covered include: restricted Boltzmann machines (RBMs) and their multi-layer extensions Deep Belief Networks and Deep Boltzmann machines; sparse coding, autoencoders, variational autoencoders, convolutional neural networks, recurrent neural networks, generative adversarial networks, and attention-based models with applications in vision, NLP, and multimodal learning. We will also address mathematical issues, focusing on efficient large-scale optimization methods for inference and learning, as well as training density models with intractable partition functions.  Prerequisite: ML: 10-701 or 10-715, and strong programming skills.
MLG10821 | Data Analysis Project Preparation | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10821&SEMESTER=S17 | instructors:Maxion, Roy description:This course is designed to help students gain a number of abilities in terms of developing a research project centered around acquiring and using real-world data, vetting that data, and forging and executing a well-founded research report.  Elements of the course include formulating a clear research hypothesis or problem statement; determining the best approach to solving that problem; getting an in-depth understanding of the data and their source; using appropriate experimental and data-analysis methods; becoming acquainted with a literature that may be unfamiliar; performing a proper literature survey; re-formulating the problem as the research develops; working effectively with other scientists/researchers; making the research reproducible; and communicating results, both orally and in writing. Students should have a data set in hand.
MLG10822 | Presentation Skills | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10822&SEMESTER=S17 | instructors:Maxion, Roy description:This course provides a forum for students to learn and refine public speaking and technical reading skills.  The course will include brief workshops embedded throughout the semester to cover such things as effective structure of presentations and papers, how to give a short talk (think NIPS spotlights), elevator talks, structure of a research paper, conference presentations, proposal writing (think thesis and beyond), slide crafting, posters, critical evaluation, and public communications for research.  Students will be expected to prepare and present a number of practice talks throughout the semester.
MLG10910 | PhD DAP Research | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10910&SEMESTER=S17 | instructors:Gordon, Geoffrey description:Course for ML PhD students to complete their Data Analysis Project research. Register for this course in the semester you plan to present your DAP.
MLG10920 | Graduate Reading and Research | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10920&SEMESTER=S17 | instructors:Gordon, Geoffrey description:This course is for graduate students to work on research with their advisor before they propose their thesis topic.
MLG10930 | Dissertation Research | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10930&SEMESTER=S17 | instructors:Gordon, Geoffrey description:This course is for graduate students to work on their dissertation research after they have proposed their thesis topic.
MLG10935 | Practicum | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10935&SEMESTER=S17 | instructors:Gordon, Geoffrey description:This course is intended for you to gain industry research experience while using the skills you have learned in the ML curriculum and will count towards the research units for your degree.
MLG10940 | Independent Study | https://enr-apps.as.cmu.edu/open/SOC/SOCServlet/courseDetails?COURSE=10940&SEMESTER=S17 | instructors:Gordon, Geoffrey description:Independent Study to be used to work on research with faculty.
