CS224D-SERIES |    FOREWORD |  | parentid:CS224D
CS224D-PREFAC |    E |  | parentid:CS224D
CS224D-I |THE PROBLEM |  | parentid:CS224D
CS224D-1 |Introduction |  | parentid:CS224D
CS224D-1.1 |    Reinforcement Learning |  | parentid:CS224D-1
CS224D-1.2 |    Examples |  | parentid:CS224D-1
CS224D-1.3 |    Elements of Reinforcement Learning |  | parentid:CS224D-1
CS224D-1.4 |    An Extended Example: Tic-Tac-Toe |  | parentid:CS224D-1
CS224D-1.5 |    Summary |  | parentid:CS224D-1
CS224D-1.6 |    History of Reinforcement Learning |  | parentid:CS224D-1
CS224D-1.7 |    Bibliographical Remarks |  | parentid:CS224D-1
CS224D-2 |Evaluative Feedback |  | parentid:CS224D
CS224D-2.1 |    An n-Armed Bandit Problem |  | parentid:CS224D-2
CS224D-2.2 |    Action-Value Methods |  | parentid:CS224D-2
CS224D-2.3 |    Softmax Action Selection |  | parentid:CS224D-2
CS224D-2.4 |    Evaluation Versus Instruction |  | parentid:CS224D-2
CS224D-2.5 |    Incremental Implementation |  | parentid:CS224D-2
CS224D-2.6 |    Tracking a Nonstationary Problem |  | parentid:CS224D-2
CS224D-2.7 |    Optimistic Initial Values |  | parentid:CS224D-2
CS224D-2.8 |    Reinforcement Comparison |  | parentid:CS224D-2
CS224D-2.9 |    Pursuit Methods |  | parentid:CS224D-2
CS224D-2.10 |    Associative Search |  | parentid:CS224D-2
CS224D-2.11 |    Conclusions |  | parentid:CS224D-2
CS224D-2.12 |    Bibliographical and Historical Remarks |  | parentid:CS224D-2
CS224D-3 |The Reinforcement Learning Problem |  | parentid:CS224D
CS224D-3.1 |    The Agent-Environment Interface |  | parentid:CS224D-3
CS224D-3.2 |    Goals and Rewards |  | parentid:CS224D-3
CS224D-3.3 |    Returns |  | parentid:CS224D-3
CS224D-3.5 |    The Markov Property |  | parentid:CS224D-3
CS224D-3.6 |    Markov Decision Processes |  | parentid:CS224D-3
CS224D-3.7 |    Value Functions |  | parentid:CS224D-3
CS224D-3.8 |    Optimal Value Functions |  | parentid:CS224D-3
CS224D-3.9 |    Optimality and Approximation |  | parentid:CS224D-3
CS224D-3.10 |    Summary |  | parentid:CS224D-3
CS224D-3.11 |    Bibliographical and Historical Remarks |  | parentid:CS224D-3
CS224D-II |    ELEMENTARY SOLUTION METHODS |  | parentid:CS224D
CS224D-4 |Dynamic Programming |  | parentid:CS224D
CS224D-4.1 |    Policy Evaluation |  | parentid:CS224D-4
CS224D-4.2 |    Policy Improvement |  | parentid:CS224D-4
CS224D-4.3 |    Policy Iteration |  | parentid:CS224D-4
CS224D-4.4 |    Value Iteration |  | parentid:CS224D-4
CS224D-4.5 |    Asynchronous Dynamic Programming |  | parentid:CS224D-4
CS224D-4.6 |    Generalized Policy Iteration |  | parentid:CS224D-4
CS224D-4.7 |    Efficiency of Dynamic Programming |  | parentid:CS224D-4
CS224D-4.8 |    Summary |  | parentid:CS224D-4
CS224D-4.9 |    Bibliographical and Historical Remarks |  | parentid:CS224D-4
CS224D-5 |Monte Carlo Methods |  | parentid:CS224D
CS224D-5.1 |    Monte Carlo Policy Evaluation |  | parentid:CS224D-5
CS224D-5.2 |    Monte Carlo Estimation of Action Values |  | parentid:CS224D-5
CS224D-5.3 |    Monte Carlo Control |  | parentid:CS224D-5
CS224D-5.4 |    On-Policy Monte Carlo Control |  | parentid:CS224D-5
CS224D-5.5 |    Evaluating One Policy While Following Another |  | parentid:CS224D-5
CS224D-5.6 |    Off-Policy Monte Carlo Control |  | parentid:CS224D-5
CS224D-5.7 |    Incremental Implementation |  | parentid:CS224D-5
CS224D-5.8 |    Summary |  | parentid:CS224D-5
CS224D-5.9 |    Bibliographical and Historical Remarks |  | parentid:CS224D-5
CS224D-6 |Temporal-Difference Learning |  | parentid:CS224D
CS224D-6.1 |    TD Prediction |  | parentid:CS224D-6
CS224D-6.2 |    Advantages of TD Prediction Methods |  | parentid:CS224D-6
CS224D-6.3 |    Optimality of TD(O) |  | parentid:CS224D-6
CS224D-6.4 |    Sarsa: On-Policy TD Control |  | parentid:CS224D-6
CS224D-6.5 |    Q-Learning: Off-Policy TD Control |  | parentid:CS224D-6
CS224D-6.6 |    Actor-Critic Methods |  | parentid:CS224D-6
CS224D-6.7 |    R-Learning for Undiscounted Continuing Tasks |  | parentid:CS224D-6
CS224D-6.8 |    Games, Afterstates, and Other Special Cases |  | parentid:CS224D-6
CS224D-6.9 |    Summary |  | parentid:CS224D-6
CS224D-6.10 |    Bibliographical and Historical Remarks |  | parentid:CS224D-6
CS224D-III |    A UNIFIED VIEW |  | parentid:CS224D
CS224D-7 |Eligibility Traces |  | parentid:CS224D
CS224D-7.1 |    n-Step TD Prediction |  | parentid:CS224D-7
CS224D-7.2 |    The Forward View of TD(l) |  | parentid:CS224D-7
CS224D-7.3 |    The Backward View of TD(l) |  | parentid:CS224D-7
CS224D-7.4 |    Equivalence of Forward and Backward Views |  | parentid:CS224D-7
CS224D-7.5 |    Sarsa(l) |  | parentid:CS224D-7
CS224D-7.6 |    Q(l) |  | parentid:CS224D-7
CS224D-7.7 |    Eligibility Traces for Actor-Critic Methods |  | parentid:CS224D-7
CS224D-7.8 |    Replacing Traces |  | parentid:CS224D-7
CS224D-7.9 |    Implementation Issues |  | parentid:CS224D-7
CS224D-7.10 |    Variable l |  | parentid:CS224D-7
CS224D-7.11 |    Conclusions |  | parentid:CS224D-7
CS224D-7.12 |    Bibliographical and Historical Remarks |  | parentid:CS224D-7
CS224D-8 |Generalization and Function Approximation |  | parentid:CS224D
CS224D-8.1 |    Value Prediction with Function Approximation |  | parentid:CS224D-8
CS224D-8.2 |    Gradient-Descent Methods |  | parentid:CS224D-8
CS224D-8.3 |    Linear Methods |  | parentid:CS224D-8
CS224D-8.4 |    Control with Function Approximation |  | parentid:CS224D-8
CS224D-8.5 |    Off-Policy Bootstrapping |  | parentid:CS224D-8
CS224D-8.6 |    Should We Bootstrap? |  | parentid:CS224D-8
CS224D-8.7 |    Summary |  | parentid:CS224D-8
CS224D-8.8 |    Bibliographical and Historical Remarks |  | parentid:CS224D-8
CS224D-9 |Planning and Learning |  | parentid:CS224D
CS224D-9.1 |    Models and Planning |  | parentid:CS224D-9
CS224D-9.2 |    Integrating Planning, Acting, and Learning |  | parentid:CS224D-9
CS224D-9.3 |    When the Model Is Wrong |  | parentid:CS224D-9
CS224D-9.4 |    Prioritized Sweeping |  | parentid:CS224D-9
CS224D-9.5 |    Full vs. Sample Backups |  | parentid:CS224D-9
CS224D-9.6 |    Trajectory Sampling |  | parentid:CS224D-9
CS224D-9.7 |    Heuristic Search |  | parentid:CS224D-9
CS224D-9.8 |    Summary |  | parentid:CS224D-9
CS224D-9.9 |    Bibliographical and Historical Remarks |  | parentid:CS224D-9
CS224D-10 |    Dimensions of Reinforcement Learning |  | parentid:CS224D
CS224D-10.1 |    The Unified View |  | parentid:CS224D-10
CS224D-10.2 |    Other Frontier Dimensions |  | parentid:CS224D-10
CS224D-11 |    Case Studies |  | parentid:CS224D
CS224D-11.1 |    TD-Gammon |  | parentid:CS224D-11
CS224D-11.2 |    Samuel s Checkers Player |  | parentid:CS224D-11
CS224D-11.3 |    The Acrobot |  | parentid:CS224D-11
CS224D-11.4 |    Elevator Dispatching |  | parentid:CS224D-11
CS224D-11.5 |    Dynamic Channel Allocation |  | parentid:CS224D-11
CS224D-11.6 |    Job-Shop Scheduling |  | parentid:CS224D-11
CS224D-REFERENCE |    S |  | parentid:CS224D
CS224D-SUMMARY |    OF NOTATION |  | parentid:CS224D
CS224D-INDE |    X |  | parentid:CS224D
